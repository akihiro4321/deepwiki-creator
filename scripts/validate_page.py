#!/usr/bin/env python3
"""
DeepWiki ãƒšãƒ¼ã‚¸å“è³ªãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼

ç”Ÿæˆã•ã‚ŒãŸ Wiki ãƒšãƒ¼ã‚¸ãŒå“è³ªåŸºæº–ã‚’æº€ãŸã—ã¦ã„ã‚‹ã‹ã‚’æ¤œè¨¼ã™ã‚‹ã€‚
å„ãƒšãƒ¼ã‚¸ã«å¯¾ã—ã¦ã‚¹ã‚³ã‚¢ã¨æ”¹å–„æŒ‡æ‘˜ã‚’å‡ºåŠ›ã™ã‚‹ã€‚
ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæŒ‡å®šæ™‚ã¯ Wiki å…¨ä½“ã®æ§‹é€ ï¼ˆãƒšãƒ¼ã‚¸æ•°ãƒ»ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç¶²ç¾…æ€§ãƒ»é…åˆ†ï¼‰ã‚‚æ¤œè¨¼ã™ã‚‹ã€‚

å“è³ªåŸºæº–ã¯ã€é–‹ç™ºè€…ãŒã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—ã‚„æ—¢å­˜æ©Ÿèƒ½ã®æ‹¡å¼µæ¤œè¨ã«
ååˆ†ãªç¶²ç¾…æ€§ãƒ»æ·±åº¦ãƒ»ã‚½ãƒ¼ã‚¹å‚ç…§å¯†åº¦ã‚’å‚™ãˆã¦ã„ã‚‹ã‹ã‚’åŸºæº–ã¨ã™ã‚‹ã€‚

ä½¿ç”¨æ–¹æ³•:
  python validate_page.py <ãƒšãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«.md> [--importance high|medium|low]
  python validate_page.py <wikiãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª> [--scale small|medium|large]
"""

import sys
import re
import os
import json
from pathlib import Path
from dataclasses import dataclass, field
from typing import Optional


@dataclass
class ValidationResult:
    """1ãƒšãƒ¼ã‚¸ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ"""
    file: str
    importance: str  # high / medium / low / unknown
    score: int = 0  # 0-100
    max_score: int = 0
    issues: list = field(default_factory=list)
    passes: list = field(default_factory=list)

    @property
    def grade(self) -> str:
        pct = (self.score / self.max_score * 100) if self.max_score > 0 else 0
        if pct >= 90:
            return "A"
        elif pct >= 75:
            return "B"
        elif pct >= 60:
            return "C"
        elif pct >= 40:
            return "D"
        else:
            return "F"

    @property
    def percentage(self) -> float:
        return (self.score / self.max_score * 100) if self.max_score > 0 else 0


# --- å“è³ªåŸºæº–å®šç¾©ï¼ˆClaude Opus ç‰ˆã‚’æ¨™æº–ï¼‰ ---
REQUIREMENTS = {
    "high": {
        "min_words": 1200,
        "min_mermaid": 2,
        "min_mermaid_types": 2,
        "min_code_snippets": 5,
        "min_sources_lines": 4,
        "sources_need_line_numbers": True,
        "min_sections": 4,
        "min_tables": 1,
    },
    "medium": {
        "min_words": 600,
        "min_mermaid": 1,
        "min_mermaid_types": 1,
        "min_code_snippets": 3,
        "min_sources_lines": 3,
        "sources_need_line_numbers": True,
        "min_sections": 3,
        "min_tables": 0,
    },
    "low": {
        "min_words": 300,
        "min_mermaid": 1,
        "min_mermaid_types": 1,
        "min_code_snippets": 1,
        "min_sources_lines": 2,
        "sources_need_line_numbers": True,
        "min_sections": 2,
        "min_tables": 0,
    },
    "index": {
        "min_words": 200,
        "min_mermaid": 1,
        "min_mermaid_types": 1,
        "min_code_snippets": 0,
        "min_sources_lines": 0,
        "sources_need_line_numbers": False,
        "min_sections": 2,
        "min_tables": 0,
    },
}

# Sources è¡Œç•ªå·ã®ç²¾åº¦ã—ãã„å€¤ï¼ˆã“ã®è¡Œæ•°ä»¥ä¸Šã®ç¯„å›²ã¯ã€Œä¸æ­£ç¢ºã€ï¼‰
MAX_ACCEPTABLE_LINE_RANGE = 200


def count_words(text: str) -> int:
    """æ—¥æœ¬èª+è‹±èªã®æ··åˆãƒ†ã‚­ã‚¹ãƒˆã®èªæ•°ã‚’æ¨å®šã€‚
    æ—¥æœ¬èª: æ–‡å­—æ•° â‰’ èªæ•°ï¼ˆåŠ©è©ç­‰å«ã‚€ï¼‰
    è‹±èª: ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Š
    """
    # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¨ Mermaid ã‚’é™¤å¤–
    cleaned = re.sub(r'```[\s\S]*?```', '', text)
    # Markdown ã®è¦‹å‡ºã—ã‚„è¨˜å·ã‚’é™¤å¤–
    cleaned = re.sub(r'[#|>\-*`\[\]()]', ' ', cleaned)

    # æ—¥æœ¬èªæ–‡å­—ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    jp_chars = len(re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]', cleaned))
    # è‹±èªå˜èªã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    en_words = len(re.findall(r'[a-zA-Z]+', cleaned))

    return jp_chars + en_words


def count_mermaid_diagrams(text: str) -> int:
    return len(re.findall(r'```mermaid', text))


def get_mermaid_types(text: str) -> set:
    """ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ Mermaid ãƒ€ã‚¤ã‚¢ã‚°ãƒ©ãƒ ã®ç¨®é¡ã‚’è¿”ã™"""
    types = set()
    mermaid_blocks = re.findall(r'```mermaid\n([\s\S]*?)```', text)
    for block in mermaid_blocks:
        first_line = block.strip().split('\n')[0].strip().lower()
        if first_line.startswith('graph'):
            types.add('graph')
        elif first_line.startswith('flowchart'):
            types.add('flowchart')
        elif first_line.startswith('sequencediagram'):
            types.add('sequenceDiagram')
        elif first_line.startswith('classdiagram'):
            types.add('classDiagram')
        elif first_line.startswith('statediagram'):
            types.add('stateDiagram')
        elif first_line.startswith('erdiagram'):
            types.add('erDiagram')
        elif first_line.startswith('gantt'):
            types.add('gantt')
        elif first_line.startswith('pie'):
            types.add('pie')
        else:
            types.add('other')
    return types


def count_code_snippets(text: str) -> int:
    """Mermaid ä»¥å¤–ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    all_blocks = re.findall(r'```(\w*)', text)
    return sum(1 for lang in all_blocks if lang and lang != 'mermaid')


def count_snippet_citations(text: str) -> int:
    """ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆå†…ã®å‡ºå…¸ã‚³ãƒ¡ãƒ³ãƒˆ (// path:Lè¡Œç•ªå·) ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    code_blocks = re.findall(r'```\w+\n([\s\S]*?)```', text)
    citations = 0
    for block in code_blocks:
        # ãƒ‘ã‚¿ãƒ¼ãƒ³: // path/to/file.ts Lè¡Œç•ªå· or // path/to/file.ts:Lè¡Œç•ªå·
        if re.search(r'//\s*\S+\.(ts|js|py|go|rs|java|tsx|jsx)\s*[:\s]L\d+', block):
            citations += 1
    return citations


def count_tables(text: str) -> int:
    """Markdown ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆãƒ˜ãƒƒãƒ€è¡Œ + åŒºåˆ‡ã‚Šè¡Œã®ãƒšã‚¢ã§åˆ¤å®šï¼‰"""
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã¯ | header | header | ã®ã‚ˆã†ãªè¡Œã®å¾Œã« | --- | --- | ãŒç¶šã
    lines = text.split('\n')
    table_count = 0
    for i in range(len(lines) - 1):
        if re.match(r'\s*\|.*\|.*\|', lines[i]) and \
           re.match(r'\s*\|[\s\-:]+\|[\s\-:]+\|', lines[i + 1]):
            table_count += 1
    return table_count


def find_sources_lines(text: str) -> list:
    """Sources: è¡Œã‚’å…¨ã¦æŠ½å‡º"""
    return re.findall(r'^.*Sources?:.*$', text, re.MULTILINE)


def check_line_numbers_in_sources(sources_lines: list) -> tuple:
    """Sources è¡Œã«è¡Œç•ªå· (Læ•°å­—) ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ã€‚ç²¾åº¦ã‚‚ãƒã‚§ãƒƒã‚¯ã€‚"""
    with_line_nums = 0
    with_imprecise_line_nums = 0
    without_line_nums = 0

    for line in sources_lines:
        ranges = re.findall(r'L(\d+)[-â€“]L?(\d+)', line)
        if ranges:
            precise = True
            for start, end in ranges:
                span = int(end) - int(start)
                if span > MAX_ACCEPTABLE_LINE_RANGE:
                    precise = False
                    break
            if precise:
                with_line_nums += 1
            else:
                with_imprecise_line_nums += 1
        elif re.search(r'L\d+', line):
            with_line_nums += 1
        else:
            without_line_nums += 1

    return with_line_nums, with_imprecise_line_nums, without_line_nums


def count_sections(text: str) -> int:
    """## ãƒ¬ãƒ™ãƒ«ã®è¦‹å‡ºã—æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    return len(re.findall(r'^## ', text, re.MULTILINE))


def check_mermaid_has_real_names(text: str) -> tuple:
    """Mermaid ãƒ€ã‚¤ã‚¢ã‚°ãƒ©ãƒ å†…ã«å…·ä½“çš„ãªã‚¯ãƒ©ã‚¹åãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹"""
    mermaid_blocks = re.findall(r'```mermaid\n([\s\S]*?)```', text)
    generic_names = {'Component', 'Module', 'Service', 'System', 'Client', 'Server',
                     'Manager', 'Handler', 'Engine', 'Registry', 'Controller'}
    has_specific = 0
    has_generic = 0
    for block in mermaid_blocks:
        labels = re.findall(r'\[([^\]]+)\]', block)
        for label in labels:
            clean = label.strip('"').strip()
            # 2èªä»¥ä¸Š or PascalCase ãªã‚‰å…·ä½“çš„
            if re.match(r'[A-Z][a-z]+[A-Z]', clean) or len(clean.split()) >= 2:
                has_specific += 1
            elif clean in generic_names:
                has_generic += 1
    return has_specific, has_generic


def check_related_pages(text: str) -> bool:
    """é–¢é€£ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ãŒã‚ã‚‹ã‹"""
    return bool(re.search(r'(é–¢é€£ãƒšãƒ¼ã‚¸|Related|â† å‰|â†’ æ¬¡|å‚ç…§)', text, re.IGNORECASE))


def check_overview_paragraph(text: str) -> bool:
    """å†’é ­ã«æ¦‚è¦æ®µè½ãŒã‚ã‚‹ã‹ï¼ˆæœ€åˆã® ## ã®å‰ã«ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹ã‹ï¼‰"""
    lines = text.split('\n')
    found_h1 = False
    has_overview = False
    for line in lines:
        if line.startswith('# ') and not line.startswith('## '):
            found_h1 = True
            continue
        if found_h1 and line.startswith('## '):
            break
        if found_h1 and line.strip() and not line.startswith('#') and not line.startswith('```') and not line.startswith('>'):
            has_overview = True
    return has_overview


def detect_importance(filepath: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰importanceã‚’æ¨æ¸¬ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ç•ªå·ãƒ™ãƒ¼ã‚¹ï¼‰"""
    basename = os.path.basename(filepath)
    if basename == 'index.md':
        return 'index'

    # x.y å½¢å¼ã®ç•ªå·ã‚’æŠ½å‡º
    match = re.match(r'(\d+)\.(\d+)', basename)
    if match:
        section = int(match.group(1))
        # Overview (1.x), Core Systems (4.x) ã¯ high
        if section in (1, 4):
            return 'high'
        # ãã®ä»–ã¯ medium
        else:
            return 'medium'

    return 'medium'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ


def validate_page(filepath: str, importance: Optional[str] = None) -> ValidationResult:
    """1ãƒšãƒ¼ã‚¸ã‚’æ¤œè¨¼"""
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()

    if importance is None:
        importance = detect_importance(filepath)

    reqs = REQUIREMENTS.get(importance, REQUIREMENTS['medium'])
    result = ValidationResult(file=filepath, importance=importance)

    # --- 1. èªæ•°ãƒã‚§ãƒƒã‚¯ (15ç‚¹) ---
    result.max_score += 15
    word_count = count_words(content)
    min_words = reqs['min_words']
    if word_count >= min_words:
        result.score += 15
        result.passes.append(f"âœ… èªæ•°: {word_count} (åŸºæº–: {min_words}ä»¥ä¸Š)")
    elif word_count >= min_words * 0.7:
        result.score += 8
        result.issues.append(f"âš ï¸  èªæ•°ä¸è¶³: {word_count} (åŸºæº–: {min_words}ä»¥ä¸Š, 70%ä»¥ä¸Šãªã®ã§éƒ¨åˆ†ç‚¹)")
    else:
        result.issues.append(f"âŒ èªæ•°ä¸è¶³: {word_count} (åŸºæº–: {min_words}ä»¥ä¸Š)")

    # --- 2. Mermaid ãƒ€ã‚¤ã‚¢ã‚°ãƒ©ãƒ æ•° (10ç‚¹) ---
    result.max_score += 10
    mermaid_count = count_mermaid_diagrams(content)
    min_mermaid = reqs['min_mermaid']
    if mermaid_count >= min_mermaid:
        result.score += 10
        result.passes.append(f"âœ… Mermaid: {mermaid_count}å€‹ (åŸºæº–: {min_mermaid}ä»¥ä¸Š)")
    elif mermaid_count > 0:
        result.score += 5
        result.issues.append(f"âš ï¸  Mermaidä¸è¶³: {mermaid_count}å€‹ (åŸºæº–: {min_mermaid}ä»¥ä¸Š)")
    else:
        result.issues.append(f"âŒ Mermaidãªã— (åŸºæº–: {min_mermaid}ä»¥ä¸Š)")

    # --- 3. Mermaid ç¨®é¡ã®å¤šæ§˜æ€§ (5ç‚¹) ---
    result.max_score += 5
    mermaid_types = get_mermaid_types(content)
    min_types = reqs['min_mermaid_types']
    if len(mermaid_types) >= min_types:
        result.score += 5
        result.passes.append(f"âœ… Mermaidç¨®é¡: {', '.join(sorted(mermaid_types))} ({len(mermaid_types)}ç¨®é¡, åŸºæº–: {min_types}ä»¥ä¸Š)")
    elif len(mermaid_types) > 0:
        result.score += 2
        result.issues.append(f"âš ï¸  Mermaidç¨®é¡ä¸è¶³: {', '.join(sorted(mermaid_types))} ({len(mermaid_types)}ç¨®é¡, åŸºæº–: {min_types}ä»¥ä¸Š)")
    else:
        result.issues.append(f"âŒ Mermaidãªã—")

    # --- 4. ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆæ•° (15ç‚¹) ---
    result.max_score += 15
    snippet_count = count_code_snippets(content)
    min_snippets = reqs['min_code_snippets']
    if snippet_count >= min_snippets:
        result.score += 15
        result.passes.append(f"âœ… ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆ: {snippet_count}å€‹ (åŸºæº–: {min_snippets}ä»¥ä¸Š)")
    elif snippet_count > 0 and min_snippets > 0:
        # å‰²åˆã«å¿œã˜ãŸéƒ¨åˆ†ç‚¹
        partial = min(10, int(15 * snippet_count / min_snippets))
        result.score += partial
        result.issues.append(f"âš ï¸  ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆä¸è¶³: {snippet_count}å€‹ (åŸºæº–: {min_snippets}ä»¥ä¸Š)")
    elif min_snippets > 0:
        result.issues.append(f"âŒ ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆãªã— (åŸºæº–: {min_snippets}ä»¥ä¸Š)")
    else:
        result.score += 15
        result.passes.append(f"âœ… ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆ: ä¸è¦ (importance: low)")

    # --- 5. ã‚¹ãƒ‹ãƒšãƒƒãƒˆå‡ºå…¸ã‚³ãƒ¡ãƒ³ãƒˆ (5ç‚¹) ---
    result.max_score += 5
    if snippet_count > 0:
        citation_count = count_snippet_citations(content)
        if citation_count >= snippet_count * 0.6:
            result.score += 5
            result.passes.append(f"âœ… ã‚¹ãƒ‹ãƒšãƒƒãƒˆå‡ºå…¸: {citation_count}/{snippet_count}å€‹ã«å‡ºå…¸ã‚³ãƒ¡ãƒ³ãƒˆã‚ã‚Š")
        elif citation_count > 0:
            result.score += 2
            result.issues.append(f"âš ï¸  ã‚¹ãƒ‹ãƒšãƒƒãƒˆå‡ºå…¸ä¸è¶³: {citation_count}/{snippet_count}å€‹ã®ã¿å‡ºå…¸ã‚ã‚Š (60%ä»¥ä¸ŠãŒåŸºæº–)")
        else:
            result.issues.append(f"âŒ ã‚¹ãƒ‹ãƒšãƒƒãƒˆå‡ºå…¸ãªã— (// path/to/file.ts:Lè¡Œç•ªå· å½¢å¼ã®ã‚³ãƒ¡ãƒ³ãƒˆãŒå¿…è¦)")
    else:
        result.score += 0  # ã‚¹ãƒ‹ãƒšãƒƒãƒˆãŒãªã‘ã‚Œã°å‡ºå…¸ã‚‚ãƒã‚§ãƒƒã‚¯ã—ãªã„

    # --- 6. Sources è¡Œå­˜åœ¨ (10ç‚¹) ---
    result.max_score += 10
    sources_lines = find_sources_lines(content)
    min_sources = reqs['min_sources_lines']
    if len(sources_lines) >= min_sources:
        result.score += 10
        result.passes.append(f"âœ… Sourcesè¡Œ: {len(sources_lines)}è¡Œ (åŸºæº–: {min_sources}ä»¥ä¸Š)")
    elif len(sources_lines) > 0:
        result.score += 5
        result.issues.append(f"âš ï¸  Sourcesè¡Œä¸è¶³: {len(sources_lines)}è¡Œ (åŸºæº–: {min_sources}ä»¥ä¸Š)")
    else:
        result.issues.append(f"âŒ Sourcesè¡Œãªã— (åŸºæº–: {min_sources}ä»¥ä¸Š)")

    # --- 7. Sources è¡Œç•ªå·ç²¾åº¦ (10ç‚¹) ---
    result.max_score += 10
    if sources_lines and reqs['sources_need_line_numbers']:
        precise, imprecise, no_ln = check_line_numbers_in_sources(sources_lines)
        total_with_any = precise + imprecise
        if precise > 0 and imprecise == 0 and no_ln == 0:
            result.score += 10
            result.passes.append(f"âœ… Sourcesè¡Œç•ªå·: å…¨{precise}è¡Œã«æ­£ç¢ºãªè¡Œç•ªå·ã‚ã‚Š")
        elif precise > 0:
            result.score += 7
            msg_parts = []
            if imprecise > 0:
                msg_parts.append(f"ä¸æ­£ç¢º{imprecise}è¡Œ(ç¯„å›²>{MAX_ACCEPTABLE_LINE_RANGE}è¡Œ)")
            if no_ln > 0:
                msg_parts.append(f"è¡Œç•ªå·ãªã—{no_ln}è¡Œ")
            result.issues.append(f"âš ï¸  Sourcesè¡Œç•ªå·: æ­£ç¢º{precise}è¡Œ, {', '.join(msg_parts)}")
        elif imprecise > 0:
            result.score += 3
            result.issues.append(f"âš ï¸  Sourcesè¡Œç•ªå·ãŒä¸æ­£ç¢º: {imprecise}è¡ŒãŒ{MAX_ACCEPTABLE_LINE_RANGE}è¡Œè¶…ã®åºƒç¯„å›² (ä¾‹: L1-L1000 ã¯ä¸å¯)")
        else:
            result.issues.append(f"âŒ Sourcesè¡Œã«è¡Œç•ªå·ãªã— (ä¾‹: [file.ts:L100-L200])")
    elif sources_lines:
        result.score += 5  # è¡Œç•ªå·ä¸è¦ã®å ´åˆã®éƒ¨åˆ†ç‚¹

    # --- 8. ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•° (5ç‚¹) ---
    result.max_score += 5
    section_count = count_sections(content)
    min_sections = reqs['min_sections']
    if section_count >= min_sections:
        result.score += 5
        result.passes.append(f"âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°: {section_count} (åŸºæº–: {min_sections}ä»¥ä¸Š)")
    else:
        result.issues.append(f"âš ï¸  ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä¸è¶³: {section_count} (åŸºæº–: {min_sections}ä»¥ä¸Š)")
        result.score += 2 if section_count > 0 else 0

    # --- 9. æ¦‚è¦æ®µè½ (5ç‚¹) ---
    result.max_score += 5
    if check_overview_paragraph(content):
        result.score += 5
        result.passes.append("âœ… æ¦‚è¦æ®µè½ã‚ã‚Š")
    else:
        result.issues.append("âŒ æ¦‚è¦æ®µè½ãªã— (# è¦‹å‡ºã—ã®ç›´å¾Œã«ã‚¹ã‚³ãƒ¼ãƒ—èª¬æ˜ãŒå¿…è¦)")

    # --- 10. ãƒ€ã‚¤ã‚¢ã‚°ãƒ©ãƒ ã®å…·ä½“æ€§ (5ç‚¹) ---
    result.max_score += 5
    if mermaid_count > 0:
        specific, generic = check_mermaid_has_real_names(content)
        if specific > 0:
            result.score += 5
            result.passes.append(f"âœ… Mermaidå†…ã«å…·ä½“çš„ãªåå‰: {specific}å€‹")
        elif generic > 0:
            result.score += 2
            result.issues.append(f"âš ï¸  Mermaidå†…ãŒæ±ç”¨åã®ã¿ ({generic}å€‹) â†’ å®Ÿéš›ã®ã‚¯ãƒ©ã‚¹åã‚’ä½¿ç”¨")
    else:
        result.score += 0

    # --- 11. é–¢é€£ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ (5ç‚¹) ---
    result.max_score += 5
    if check_related_pages(content):
        result.score += 5
        result.passes.append("âœ… é–¢é€£ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ã‚ã‚Š")
    else:
        result.issues.append("âš ï¸  é–¢é€£ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ãªã—")

    # --- 12. ãƒ†ãƒ¼ãƒ–ãƒ« (5ç‚¹) ---
    result.max_score += 5
    table_count = count_tables(content)
    min_tables = reqs['min_tables']
    if min_tables > 0:
        if table_count >= min_tables:
            result.score += 5
            result.passes.append(f"âœ… ãƒ†ãƒ¼ãƒ–ãƒ«: {table_count}å€‹ (åŸºæº–: {min_tables}ä»¥ä¸Š)")
        elif table_count > 0:
            result.score += 2
            result.issues.append(f"âš ï¸  ãƒ†ãƒ¼ãƒ–ãƒ«ä¸è¶³: {table_count}å€‹ (åŸºæº–: {min_tables}ä»¥ä¸Š)")
        else:
            result.issues.append(f"âŒ ãƒ†ãƒ¼ãƒ–ãƒ«ãªã— (åŸºæº–: {min_tables}ä»¥ä¸Š, åˆ—æŒ™å‹ãƒ»å®šæ•°ãƒ»ã‚«ãƒ†ã‚´ãƒªã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã§æ•´ç†)")
    else:
        if table_count > 0:
            result.score += 5
            result.passes.append(f"âœ… ãƒ†ãƒ¼ãƒ–ãƒ«: {table_count}å€‹ (æ¨å¥¨)")
        else:
            result.score += 3  # medium/low ã§ã¯ãƒ†ãƒ¼ãƒ–ãƒ«ãªã—ã§ã‚‚è¨±å®¹
            result.passes.append(f"âœ… ãƒ†ãƒ¼ãƒ–ãƒ«: ä»»æ„ (importance: {importance})")

    return result


def format_result(result: ValidationResult) -> str:
    """çµæœã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    lines = []
    basename = os.path.basename(result.file)
    lines.append(f"{'='*60}")
    lines.append(f"ğŸ“„ {basename}")
    lines.append(f"   Importance: {result.importance}  |  Grade: {result.grade}  |  Score: {result.score}/{result.max_score} ({result.percentage:.0f}%)")
    lines.append(f"{'='*60}")

    if result.issues:
        lines.append("")
        lines.append("  æ”¹å–„ãŒå¿…è¦:")
        for issue in result.issues:
            lines.append(f"    {issue}")

    if result.passes:
        lines.append("")
        lines.append("  åˆæ ¼é …ç›®:")
        for p in result.passes:
            lines.append(f"    {p}")

    lines.append("")
    return '\n'.join(lines)


def format_summary(results: list) -> str:
    """å…¨ä½“ã‚µãƒãƒªãƒ¼ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    lines = []
    lines.append(f"\n{'#'*60}")
    lines.append(f"  DeepWiki å“è³ªãƒ¬ãƒãƒ¼ãƒˆ")
    lines.append(f"{'#'*60}\n")

    total_score = sum(r.score for r in results)
    total_max = sum(r.max_score for r in results)
    avg_pct = (total_score / total_max * 100) if total_max > 0 else 0

    # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ
    grades = {}
    for r in results:
        g = r.grade
        grades[g] = grades.get(g, 0) + 1

    lines.append(f"  ç·åˆã‚¹ã‚³ã‚¢: {total_score}/{total_max} ({avg_pct:.0f}%)")
    lines.append(f"  ãƒšãƒ¼ã‚¸æ•°: {len(results)}")
    lines.append(f"  ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ: {', '.join(f'{g}={c}' for g, c in sorted(grades.items()))}")
    lines.append("")

    # ãƒšãƒ¼ã‚¸åˆ¥ã‚µãƒãƒªãƒ¼è¡¨
    lines.append(f"  {'ãƒšãƒ¼ã‚¸':<45} {'Grade':>5}  {'Score':>10}")
    lines.append(f"  {'-'*45} {'-'*5}  {'-'*10}")
    for r in results:
        basename = os.path.basename(r.file)
        lines.append(f"  {basename:<45} {r.grade:>5}  {r.score:>3}/{r.max_score:<3} ({r.percentage:.0f}%)")

    # ä¸åˆæ ¼ãƒšãƒ¼ã‚¸
    failing = [r for r in results if r.grade in ('D', 'F')]
    if failing:
        lines.append(f"\n  âš ï¸  è¦æ”¹å–„ãƒšãƒ¼ã‚¸ ({len(failing)}ä»¶):")
        for r in failing:
            basename = os.path.basename(r.file)
            top_issues = [i for i in r.issues if i.startswith('âŒ')][:3]
            lines.append(f"    - {basename}: {', '.join(top_issues)}")

    lines.append("")
    return '\n'.join(lines)


# --- Wiki å…¨ä½“æ§‹é€ ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ ---

# è¦æ¨¡åˆ¥ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼ˆSKILL.md Phase 2 ã¨åŒæœŸï¼‰
SCALE_GUIDELINES = {
    "small": {
        "label": "å°è¦æ¨¡",
        "file_count": "<30",
        "min_sections": 3,
        "max_sections": 4,
        "min_pages": 8,
        "max_pages": 15,
    },
    "medium": {
        "label": "ä¸­è¦æ¨¡",
        "file_count": "30-200",
        "min_sections": 4,
        "max_sections": 6,
        "min_pages": 15,
        "max_pages": 30,
    },
    "large": {
        "label": "å¤§è¦æ¨¡",
        "file_count": ">200",
        "min_sections": 6,
        "max_sections": 8,
        "min_pages": 30,
        "max_pages": 50,
    },
}

# å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆSKILL.md ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹æˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«åŸºã¥ãï¼‰
SECTION_DEFINITIONS = {
    1: {"name": "Overview", "required": True, "desc": "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦ãƒ»ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ"},
    2: {"name": "Getting Started", "required": True, "desc": "ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ»èªè¨¼ãƒ»è¨­å®š"},
    3: {"name": "User Guide", "required": True, "desc": "ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘æ©Ÿèƒ½ï¼ˆCLIã€UIã€æ“ä½œæ–¹æ³•ï¼‰"},
    4: {"name": "Core Systems", "required": True, "desc": "å†…éƒ¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ä¸»è¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"},
    5: {"name": "Advanced Topics", "required": False, "desc": "æ‹¡å¼µæ€§ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ»ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãƒ»å¯è¦³æ¸¬æ€§"},
    6: {"name": "Development", "required": False, "desc": "é–‹ç™ºç’°å¢ƒãƒ»ãƒ“ãƒ«ãƒ‰ãƒ»ãƒ†ã‚¹ãƒˆ"},
}


def infer_scale(page_count: int) -> str:
    """ãƒšãƒ¼ã‚¸æ•°ã‹ã‚‰è¦æ¨¡ã‚’æ¨å®š"""
    if page_count <= 15:
        return "small"
    elif page_count <= 30:
        return "medium"
    else:
        return "large"


def analyze_sections(results: list) -> dict:
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹é€ ã‚’åˆ†æ"""
    sections = {}  # section_num -> list of page basenames
    unclassified = []

    for r in results:
        basename = os.path.basename(r.file)
        if basename == 'index.md':
            continue
        # x.y-name.md or x-name.md
        match = re.match(r'^(\d+)', basename)
        if match:
            section_num = int(match.group(1))
            if section_num not in sections:
                sections[section_num] = []
            sections[section_num].append(basename)
        else:
            unclassified.append(basename)

    return sections, unclassified


@dataclass
class WikiStructureResult:
    """Wikiå…¨ä½“æ§‹é€ ã®æ¤œè¨¼çµæœ"""
    scale: str
    page_count: int
    issues: list = field(default_factory=list)
    passes: list = field(default_factory=list)
    score: int = 0
    max_score: int = 0


def validate_wiki_structure(results: list, scale: Optional[str] = None) -> WikiStructureResult:
    """Wikiå…¨ä½“ã®æ§‹é€ ã‚’æ¤œè¨¼ï¼ˆãƒšãƒ¼ã‚¸æ•°ãƒ»ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç¶²ç¾…æ€§ãƒ»é…åˆ†ï¼‰"""
    page_count = len([r for r in results if os.path.basename(r.file) != 'index.md'])

    if scale is None:
        scale = infer_scale(page_count)

    guide = SCALE_GUIDELINES[scale]
    ws = WikiStructureResult(scale=scale, page_count=page_count)
    sections, unclassified = analyze_sections(results)

    # --- 1. ç·ãƒšãƒ¼ã‚¸æ•°ãƒã‚§ãƒƒã‚¯ (20ç‚¹) ---
    ws.max_score += 20
    min_p, max_p = guide['min_pages'], guide['max_pages']
    if min_p <= page_count <= max_p:
        ws.score += 20
        ws.passes.append(f"âœ… ãƒšãƒ¼ã‚¸æ•°: {page_count} (è¦æ¨¡'{guide['label']}': {min_p}-{max_p}ãƒšãƒ¼ã‚¸)")
    elif page_count > max_p:
        ws.score += 15  # ãƒšãƒ¼ã‚¸å¤šã„åˆ†ã¯æ¸›ç‚¹å°‘ãªã‚
        ws.issues.append(f"âš ï¸  ãƒšãƒ¼ã‚¸æ•°ãŒå¤šã„: {page_count} (è¦æ¨¡'{guide['label']}': {min_p}-{max_p}ãƒšãƒ¼ã‚¸)")
    elif page_count >= min_p * 0.7:
        ws.score += 10
        ws.issues.append(f"âš ï¸  ãƒšãƒ¼ã‚¸æ•°ãŒã‚„ã‚„å°‘ãªã„: {page_count} (è¦æ¨¡'{guide['label']}': {min_p}-{max_p}ãƒšãƒ¼ã‚¸)")
    else:
        ws.issues.append(f"âŒ ãƒšãƒ¼ã‚¸æ•°ä¸è¶³: {page_count} (è¦æ¨¡'{guide['label']}': {min_p}ãƒšãƒ¼ã‚¸ä»¥ä¸Šå¿…è¦)")

    # --- 2. å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ç¶²ç¾…æ€§ãƒã‚§ãƒƒã‚¯ (30ç‚¹) ---
    ws.max_score += 30
    required_present = 0
    required_total = 0
    missing_required = []
    missing_optional = []

    for sec_num, sec_def in SECTION_DEFINITIONS.items():
        if sec_def['required']:
            required_total += 1
            if sec_num in sections:
                required_present += 1
            else:
                missing_required.append(f"Section {sec_num} ({sec_def['name']}): {sec_def['desc']}")
        else:
            if sec_num not in sections:
                missing_optional.append(f"Section {sec_num} ({sec_def['name']}): {sec_def['desc']}")

    if required_present == required_total:
        ws.score += 30
        ws.passes.append(f"âœ… å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³: {required_present}/{required_total} å…¨ã¦å­˜åœ¨")
    else:
        ws.score += int(30 * required_present / required_total)
        for m in missing_required:
            ws.issues.append(f"âŒ å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³æ¬ è½: {m}")

    if missing_optional:
        for m in missing_optional:
            ws.issues.append(f"âš ï¸  æ¨å¥¨ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ¬ è½: {m}")

    # --- 3. Core Systems + User Guide ã®é…åˆ†ãƒã‚§ãƒƒã‚¯ (20ç‚¹) ---
    ws.max_score += 20
    core_user_pages = len(sections.get(3, [])) + len(sections.get(4, []))
    if page_count > 0:
        ratio = core_user_pages / page_count
        if 0.40 <= ratio <= 0.70:
            ws.score += 20
            ws.passes.append(f"âœ… Core Systems + User Guide ã®é…åˆ†: {core_user_pages}/{page_count} ({ratio:.0%})")
        elif 0.30 <= ratio < 0.40:
            ws.score += 10
            ws.issues.append(f"âš ï¸  Core Systems + User Guide ã®é…åˆ†ãŒä½ã„: {core_user_pages}/{page_count} ({ratio:.0%}, æ¨å¥¨: 50-60%)")
        elif ratio > 0.70:
            ws.score += 15
            ws.issues.append(f"âš ï¸  Core Systems + User Guide ãŒå¤šã™ãã‚‹: {core_user_pages}/{page_count} ({ratio:.0%}, Overview/Getting Started ã‚‚å……å®Ÿã•ã›ã‚‹)")
        else:
            ws.issues.append(f"âŒ Core Systems + User Guide ã®é…åˆ†ãŒä¸é©åˆ‡: {core_user_pages}/{page_count} ({ratio:.0%}, æ¨å¥¨: 50-60%)")

    # --- 4. ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°ãƒã‚§ãƒƒã‚¯ (15ç‚¹) ---
    ws.max_score += 15
    section_count = len(sections)
    min_s, max_s = guide['min_sections'], guide['max_sections']
    if min_s <= section_count <= max_s:
        ws.score += 15
        ws.passes.append(f"âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°: {section_count} (è¦æ¨¡'{guide['label']}': {min_s}-{max_s})")
    elif section_count > max_s:
        ws.score += 12
        ws.issues.append(f"âš ï¸  ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°ãŒå¤šã„: {section_count} (è¦æ¨¡'{guide['label']}': {min_s}-{max_s})")
    elif section_count >= min_s - 1:
        ws.score += 8
        ws.issues.append(f"âš ï¸  ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°ãŒã‚„ã‚„å°‘ãªã„: {section_count} (è¦æ¨¡'{guide['label']}': {min_s}-{max_s})")
    else:
        ws.issues.append(f"âŒ ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°ä¸è¶³: {section_count} (è¦æ¨¡'{guide['label']}': {min_s}ä»¥ä¸Šå¿…è¦)")

    # --- 5. å“è³ªã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒãƒã‚§ãƒƒã‚¯ (15ç‚¹) ---
    ws.max_score += 15
    grade_b_plus = sum(1 for r in results if r.grade in ('A', 'B') and os.path.basename(r.file) != 'index.md')
    grade_d_f = sum(1 for r in results if r.grade in ('D', 'F') and os.path.basename(r.file) != 'index.md')
    b_plus_ratio = grade_b_plus / page_count if page_count > 0 else 0
    if b_plus_ratio >= 0.80:
        ws.score += 15
        ws.passes.append(f"âœ… Grade Bä»¥ä¸Šã®æ¯”ç‡: {grade_b_plus}/{page_count} ({b_plus_ratio:.0%})")
    elif b_plus_ratio >= 0.60:
        ws.score += 10
        ws.issues.append(f"âš ï¸  Grade Bä»¥ä¸Šã®æ¯”ç‡ãŒä½ã„: {grade_b_plus}/{page_count} ({b_plus_ratio:.0%}, ç›®æ¨™: 80%ä»¥ä¸Š)")
    else:
        ws.score += 5 if grade_b_plus > 0 else 0
        ws.issues.append(f"âŒ Grade Bä»¥ä¸Šã®æ¯”ç‡ãŒä¸è¶³: {grade_b_plus}/{page_count} ({b_plus_ratio:.0%}, ç›®æ¨™: 80%ä»¥ä¸Š)")

    if grade_d_f > 0:
        ws.issues.append(f"âŒ Grade D/F ã®ãƒšãƒ¼ã‚¸ãŒ {grade_d_f} ä»¶ã‚ã‚Šï¼ˆä¿®æ­£ãŒå¿…è¦ï¼‰")

    return ws


def format_structure_result(ws: WikiStructureResult, sections: dict) -> str:
    """æ§‹é€ ãƒã‚§ãƒƒã‚¯çµæœã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    lines = []
    lines.append(f"\n{'='*60}")
    lines.append(f"  ğŸ“Š Wiki æ§‹é€ ãƒã‚§ãƒƒã‚¯ (è¦æ¨¡: {SCALE_GUIDELINES[ws.scale]['label']})")
    lines.append(f"{'='*60}")
    pct = (ws.score / ws.max_score * 100) if ws.max_score > 0 else 0
    lines.append(f"  ã‚¹ã‚³ã‚¢: {ws.score}/{ws.max_score} ({pct:.0f}%)")
    lines.append("")

    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥ãƒšãƒ¼ã‚¸æ•°
    lines.append("  ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥ãƒšãƒ¼ã‚¸æ•°:")
    for sec_num in sorted(sections.keys()):
        sec_def = SECTION_DEFINITIONS.get(sec_num, {"name": f"Section {sec_num}", "required": False})
        req_mark = "â˜…" if sec_def.get('required') else " "
        lines.append(f"    {req_mark} Section {sec_num} ({sec_def['name']}): {len(sections[sec_num])}ãƒšãƒ¼ã‚¸")
    lines.append("")

    if ws.passes:
        lines.append("  åˆæ ¼é …ç›®:")
        for p in ws.passes:
            lines.append(f"    {p}")
        lines.append("")

    if ws.issues:
        lines.append("  æ”¹å–„ãŒå¿…è¦:")
        for i in ws.issues:
            lines.append(f"    {i}")
        lines.append("")

    return '\n'.join(lines)


def generate_ai_corrections(results: list, ws: Optional[WikiStructureResult] = None) -> str:
    """AIãƒ¢ãƒ‡ãƒ«ãŒå®Ÿè¡Œã™ã¹ãä¿®æ­£æŒ‡ç¤ºã‚’ç”Ÿæˆã™ã‚‹"""
    lines = []
    lines.append(f"\n{'='*60}")
    lines.append(f"  ğŸ¤– AIãƒ¢ãƒ‡ãƒ«å‘ã‘ä¿®æ­£æŒ‡ç¤º")
    lines.append(f"{'='*60}")

    instructions = []
    priority = 1

    # --- æ§‹é€ ãƒ¬ãƒ™ãƒ«ã®ä¿®æ­£æŒ‡ç¤º ---
    if ws:
        for issue in ws.issues:
            if 'å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³æ¬ è½' in issue:
                # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åã‚’æŠ½å‡º
                match = re.search(r'Section (\d+) \((.+?)\): (.+)', issue)
                if match:
                    sec_num, sec_name, sec_desc = match.groups()
                    instructions.append({
                        "priority": priority,
                        "type": "æ§‹é€ : ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¿½åŠ ",
                        "action": f"Section {sec_num} ({sec_name}) ã‚’æ–°è¦ä½œæˆã—ã¦ãã ã•ã„ã€‚\n"
                                  f"        å†…å®¹: {sec_desc}\n"
                                  f"        ãƒ•ã‚¡ã‚¤ãƒ«å‘½åä¾‹: {sec_num}.1-{sec_name.lower().replace(' ', '-')}.md\n"
                                  f"        ä½œæˆæ‰‹é †: Phase 3aï¼ˆã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰åˆ†æï¼‰â†’ Phase 3bï¼ˆãƒšãƒ¼ã‚¸ç”Ÿæˆï¼‰ã®é †ã§é€²ã‚ã€\n"
                                  f"        é–¢é€£ã™ã‚‹ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ view_file ã§èª­ã¿ã€ã‚¹ãƒ‹ãƒšãƒƒãƒˆå€™è£œã‚’5å€‹ä»¥ä¸Šãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ã‹ã‚‰æ›¸ãå§‹ã‚ã¦ãã ã•ã„ã€‚",
                    })
                    priority += 1

            elif 'ãƒšãƒ¼ã‚¸æ•°ä¸è¶³' in issue:
                instructions.append({
                    "priority": priority,
                    "type": "æ§‹é€ : ãƒšãƒ¼ã‚¸è¿½åŠ ",
                    "action": f"ãƒšãƒ¼ã‚¸æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚Core Systems (Section 4) ã§1ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«=1ãƒšãƒ¼ã‚¸ã®åŸå‰‡ã«æ²¿ã£ã¦åˆ†å‰²ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚\n"
                              f"        ç‰¹ã«ã€1ã¤ã®ãƒšãƒ¼ã‚¸ã§è¤‡æ•°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’èª¬æ˜ã—ã¦ã„ã‚‹å ´åˆã¯åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚\n"
                              f"        User Guide (Section 3) ã§ã‚‚ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘æ©Ÿèƒ½ã”ã¨ã«ãƒšãƒ¼ã‚¸ã‚’è¿½åŠ ã§ãã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
                })
                priority += 1

            elif 'Grade D/F' in issue:
                d_f_pages = [r for r in results if r.grade in ('D', 'F') and os.path.basename(r.file) != 'index.md']
                for r in d_f_pages:
                    basename = os.path.basename(r.file)
                    top_issues = [i for i in r.issues if i.startswith('âŒ')]
                    instructions.append({
                        "priority": priority,
                        "type": f"ãƒšãƒ¼ã‚¸ä¿®æ­£: {basename}",
                        "action": f"ã“ã®ãƒšãƒ¼ã‚¸ã¯ Grade {r.grade} ({r.percentage:.0f}%) ã§ã™ã€‚ä»¥ä¸‹ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„:\n" +
                                  "\n".join(f"        - {i}" for i in top_issues),
                    })
                    priority += 1

    # --- ãƒšãƒ¼ã‚¸ãƒ¬ãƒ™ãƒ«ã®ä¿®æ­£æŒ‡ç¤ºï¼ˆGrade C ä»¥ä¸‹ï¼‰ ---
    problem_pages = sorted(
        [r for r in results if r.grade in ('C', 'D', 'F') and os.path.basename(r.file) != 'index.md'],
        key=lambda r: r.percentage
    )

    for r in problem_pages:
        basename = os.path.basename(r.file)
        critical_issues = [i for i in r.issues if i.startswith('âŒ')]
        warning_issues = [i for i in r.issues if i.startswith('âš ï¸')]

        if not critical_issues and not warning_issues:
            continue

        action_parts = []

        for issue in critical_issues:
            if 'ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆ' in issue:
                action_parts.append(
                    "ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ view_file ã§èª­ã¿ç›´ã—ã€\n"
                    "          ä¸»è¦ã‚¯ãƒ©ã‚¹ã®å®šç¾©ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã€é‡è¦ãƒ¡ã‚½ãƒƒãƒ‰ã®ã‚·ã‚°ãƒãƒãƒ£ã‚’æŠœç²‹ã—ã¦ãã ã•ã„ã€‚\n"
                    "          å„ã‚¹ãƒ‹ãƒšãƒƒãƒˆã®å…ˆé ­ã« `// path/to/file.ts:Lè¡Œç•ªå·` ã®å‡ºå…¸ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚"
                )
            elif 'èªæ•°ä¸è¶³' in issue:
                action_parts.append(
                    "å†…å®¹ã‚’å……å®Ÿã•ã›ã¦ãã ã•ã„ã€‚å…·ä½“çš„ã«ã¯:\n"
                    "          - è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®èª¬æ˜ã¨é©ç”¨ç®‡æ‰€ã®è§£èª¬ã‚’è¿½åŠ \n"
                    "          - ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ï¼ˆå…¥åŠ›â†’å‡¦ç†â†’å‡ºåŠ›ï¼‰ã®èª¬æ˜ã‚’è¿½åŠ \n"
                    "          - ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã‚„ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®è§£èª¬ã‚’è¿½åŠ "
                )
            elif 'Mermaid' in issue:
                action_parts.append(
                    "Mermaidãƒ€ã‚¤ã‚¢ã‚°ãƒ©ãƒ ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚æ¨å¥¨: flowchartï¼ˆå‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼‰+ sequenceDiagramï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é–“é€šä¿¡ï¼‰ã®2ç¨®é¡ã€‚\n"
                    "          ãƒãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«ã«ã¯å®Ÿéš›ã®ã‚¯ãƒ©ã‚¹åãƒ»é–¢æ•°åã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
                )
            elif 'Sources' in issue:
                action_parts.append(
                    "Sourcesè¡Œã‚’å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³æœ«å°¾ã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚å½¢å¼: **Sources:** [file.ts:L100-L200](file:///path#L100-L200)\n"
                    "          è¡Œç•ªå·ã¯ view_file ã§å®Ÿéš›ã«èª­ã‚“ã ç¯„å›²ã‚’200è¡Œä»¥å†…ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
                )
            elif 'ãƒ†ãƒ¼ãƒ–ãƒ«' in issue:
                action_parts.append(
                    "ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚å¯¾è±¡: åˆ—æŒ™å‹ã®å€¤ä¸€è¦§ã€å®šæ•°ã‚°ãƒ«ãƒ¼ãƒ—ã€ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å½¹å‰²åˆ†æ‹…ã€è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç­‰ã€‚"
                )

        for issue in warning_issues:
            if 'Mermaidç¨®é¡ãŒå˜ä¸€' in issue or 'Mermaidå†…ãŒæ±ç”¨å' in issue:
                action_parts.append(
                    "Mermaidãƒ€ã‚¤ã‚¢ã‚°ãƒ©ãƒ ã®ç¨®é¡ã‚’å¢—ã‚„ã—ã¦ãã ã•ã„ã€‚graph TD ã ã‘ã§ãªã sequenceDiagram ã‚„ stateDiagram-v2 ã‚‚ä½¿ã„åˆ†ã‘ã¦ãã ã•ã„ã€‚"
                )
            elif 'è¡Œç•ªå·ãŒä¸æ­£ç¢º' in issue or 'è¡Œç•ªå·ãªã—' in issue:
                action_parts.append(
                    "Sourcesè¡Œã®è¡Œç•ªå·ã‚’æ­£ç¢ºã«ã—ã¦ãã ã•ã„ã€‚L1-L1000 ã®ã‚ˆã†ãªåºƒç¯„å›²ã¯ä¸å¯ã€‚å‚ç…§ã—ãŸé–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹ã®å®Ÿéš›ã®è¡Œç¯„å›²ã‚’200è¡Œä»¥å†…ã§è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚"
                )

        if action_parts:
            # é‡è¤‡ã—ãŸæŒ‡ç¤ºã‚’æ’é™¤
            seen = set()
            unique_parts = []
            for part in action_parts:
                key = part[:30]  # å…ˆé ­30æ–‡å­—ã§é‡è¤‡åˆ¤å®š
                if key not in seen:
                    seen.add(key)
                    unique_parts.append(part)

            instructions.append({
                "priority": priority,
                "type": f"ãƒšãƒ¼ã‚¸æ”¹å–„: {basename} (Grade {r.grade}, {r.percentage:.0f}%)",
                "action": "\n".join(f"        [{i+1}] {p}" for i, p in enumerate(unique_parts)),
            })
            priority += 1

    # --- å‡ºåŠ› ---
    if not instructions:
        lines.append("\n  âœ… ä¿®æ­£æŒ‡ç¤ºãªã— â€” å…¨ãƒšãƒ¼ã‚¸ãŒåŸºæº–ã‚’æº€ãŸã—ã¦ã„ã¾ã™ã€‚")
    else:
        lines.append(f"\n  ä¿®æ­£æŒ‡ç¤º: {len(instructions)}ä»¶ (å„ªå…ˆåº¦é †)\n")
        for inst in instructions:
            lines.append(f"  [{inst['priority']}] {inst['type']}")
            lines.append(f"      {inst['action']}")
            lines.append("")

    lines.append("")
    return '\n'.join(lines)


def main():
    if len(sys.argv) < 2:
        print(__doc__)
        sys.exit(1)

    target = sys.argv[1]
    importance_override = None
    scale_override = None

    if '--importance' in sys.argv:
        idx = sys.argv.index('--importance')
        if idx + 1 < len(sys.argv):
            importance_override = sys.argv[idx + 1]

    if '--scale' in sys.argv:
        idx = sys.argv.index('--scale')
        if idx + 1 < len(sys.argv):
            scale_override = sys.argv[idx + 1]

    # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ« or ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    if os.path.isfile(target):
        result = validate_page(target, importance_override)
        print(format_result(result))

        # ãƒšãƒ¼ã‚¸å˜ä½“ã§ã‚‚AIä¿®æ­£æŒ‡ç¤ºã‚’å‡ºã™
        if result.grade in ('C', 'D', 'F'):
            print(generate_ai_corrections([result]))

        # çµ‚äº†ã‚³ãƒ¼ãƒ‰: ä¸åˆæ ¼ãªã‚‰ 1
        sys.exit(0 if result.grade in ('A', 'B', 'C') else 1)

    elif os.path.isdir(target):
        md_files = sorted(Path(target).glob('*.md'))
        if not md_files:
            print(f"ERROR: {target} ã« .md ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            sys.exit(1)

        results = []
        for md_file in md_files:
            result = validate_page(str(md_file), importance_override)
            results.append(result)
            print(format_result(result))

        print(format_summary(results))

        # Wiki æ§‹é€ ãƒã‚§ãƒƒã‚¯
        ws = validate_wiki_structure(results, scale_override)
        sections, _ = analyze_sections(results)
        print(format_structure_result(ws, sections))

        # AI ä¿®æ­£æŒ‡ç¤º
        problem_pages = [r for r in results if r.grade in ('C', 'D', 'F')]
        if problem_pages or ws.issues:
            print(generate_ai_corrections(results, ws))

        # å…¨ãƒšãƒ¼ã‚¸ C ä»¥ä¸Šãªã‚‰æˆåŠŸ
        failing = [r for r in results if r.grade in ('D', 'F')]
        sys.exit(0 if not failing else 1)

    else:
        print(f"ERROR: {target} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)


if __name__ == '__main__':
    main()
